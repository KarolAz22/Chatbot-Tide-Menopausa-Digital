{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS: C√≥digo executado no colab\n",
    "#Se voc√™ j√° tem o arquivo doc_clean_unstructed n√£o precisa executar este arquivo\n",
    "# Instala todas as bibliotecas necess√°rias para o pipeline\n",
    "!pip install -qU langchain langchain_community langchain_core\n",
    "!pip install -qU pandas beautifulsoup4\n",
    "!pip install -qU \"unstructured[local-inference]\" unstructured_client\n",
    "!pip install -qU langchain-google-genai qdrant-client pypdf\n",
    "\n",
    "# A Unstructured precisa da biblioteca 'poppler' para processar PDFs.\n",
    "# No Colab (que √© um ambiente Linux), podemos instalar com o comando abaixo.\n",
    "!apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import CSVLoader, UnstructuredURLLoader\n",
    "\n",
    "# ======================================================\n",
    "# === FUN√á√ïES DE CACHE (Salvar e Carregar JSONL) =======\n",
    "# ======================================================\n",
    "\n",
    "def salvar_documentos_em_jsonl(documentos: List, caminho_arquivo: str):\n",
    "    \"\"\"Salva uma lista de Documentos LangChain em um arquivo.jsonl.\"\"\"\n",
    "    with open(caminho_arquivo, 'w', encoding='utf-8') as f:\n",
    "        for doc in documentos:\n",
    "            linha = json.dumps({\n",
    "                'page_content': doc.page_content,\n",
    "                'metadata': doc.metadata\n",
    "            }, ensure_ascii=False)\n",
    "            f.write(linha + '\\n')\n",
    "    print(f\"Documentos salvos em: {caminho_arquivo} ({len(documentos)} registros)\")\n",
    "\n",
    "def carregar_documentos_de_jsonl(caminho_arquivo: str) -> List:\n",
    "    \"\"\"Carrega uma lista de Documentos LangChain de um arquivo.jsonl.\"\"\"\n",
    "    documentos = []\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        for linha in f:\n",
    "            dados = json.loads(linha)\n",
    "            doc = Document(page_content=dados['page_content'], metadata=dados['metadata'])\n",
    "            documentos.append(doc)\n",
    "    print(f\"{len(documentos)} documentos carregados do cache: {caminho_arquivo}\")\n",
    "    return documentos\n",
    "\n",
    "# ======================================================\n",
    "# === FUN√á√ÉO DE LIMPEZA FINAL DE TEXTO =================\n",
    "# ======================================================\n",
    "\n",
    "def limpar_texto_final(texto: str) -> str:\n",
    "    \"\"\"Remove m√∫ltiplos espa√ßos e quebras de linha para um texto j√° limpo.\"\"\"\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    texto = texto.strip()\n",
    "    return texto\n",
    "\n",
    "# ======================================================\n",
    "# === FUN√á√ÉO PRINCIPAL DE COLETA E LIMPEZA\n",
    "# ======================================================\n",
    "\n",
    "def carregar_e_limpar_conteudo(documentos_origem: List) -> List:\n",
    "    \"\"\"\n",
    "    Faz o download e a limpeza inteligente de cada documento usando UnstructuredURLLoader.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando o processo de download e limpeza inteligente com Unstructured...\")\n",
    "    documentos_finais = []\n",
    "    urls_processadas = set()\n",
    "\n",
    "    # Contadores de tipo de documento\n",
    "    total_pdfs = 0\n",
    "    total_htmls = 0\n",
    "    total_processadas = 0\n",
    "\n",
    "    for i, doc_origem in enumerate(documentos_origem):\n",
    "        source_url = doc_origem.metadata['source']\n",
    "        if source_url in urls_processadas:\n",
    "            print(f\"‚ö†Ô∏è URL repetida ignorada: {source_url}\")\n",
    "            continue\n",
    "        urls_processadas.add(source_url)\n",
    "        print(f\"Processando URL {i+1}/{len(documentos_origem)}: {source_url}\")\n",
    "\n",
    "        try:\n",
    "            # 1. Adicionamos o par√¢metro 'languages' para otimizar para o portugu√™s.\n",
    "            # 2. Usamos a estrat√©gia 'hi_res' para PDFs para maior precis√£o.\n",
    "            is_pdf = source_url.lower().endswith(\".pdf\") or \"format=pdf\" in source_url.lower()\n",
    "            strategy = \"hi_res\" if '.pdf' in source_url.lower() else \"fast\"\n",
    "\n",
    "            loader = UnstructuredURLLoader(\n",
    "                urls=[source_url],\n",
    "                mode=\"elements\",\n",
    "                strategy=strategy,\n",
    "                unstructured_kwargs={\"languages\": [\"por\"]} # <-- INFORMA O IDIOMA\n",
    "            )\n",
    "            elementos = loader.load()\n",
    "\n",
    "            # Em vez de permitir apenas alguns tipos, vamos excluir os tipos irrelevantes.\n",
    "            tipos_irrelevantes = [\"Header\", \"Footer\", \"PageNumber\", \"Image\", \"Table\", \"FigureCaption\"]\n",
    "\n",
    "\n",
    "            texto_combinado = \"\"\n",
    "            for elemento in elementos:\n",
    "                # Mantemos o elemento se sua categoria N√ÉO ESTIVER na lista de irrelevantes\n",
    "                if elemento.metadata.get('category') not in tipos_irrelevantes:\n",
    "                    texto_combinado += elemento.page_content + \"\\n\\n\"\n",
    "\n",
    "            if len(texto_combinado) > 100:\n",
    "                texto_final_limpo = limpar_texto_final(texto_combinado)\n",
    "                metadata_final = {\n",
    "                    **doc_origem.metadata,\n",
    "                    \"tipo\": \"PDF\" if is_pdf else \"HTML\",\n",
    "                    \"num_palavras\": len(texto_final_limpo.split())\n",
    "                }\n",
    "                documentos_finais.append(Document(\n",
    "                    page_content=texto_final_limpo,\n",
    "                    metadata=doc_origem.metadata\n",
    "                ))\n",
    "\n",
    "                # Incrementa contadores\n",
    "                if is_pdf:\n",
    "                    total_pdfs += 1\n",
    "                else:\n",
    "                    total_htmls += 1\n",
    "\n",
    "                total_processadas += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"-> Erro ao processar {source_url} com Unstructured: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ {len(documentos_finais)} documentos coletados e limpos com sucesso.\")\n",
    "\n",
    "    # ======================================================\n",
    "    # === RELAT√ìRIO FINAL ==================================\n",
    "    # ======================================================\n",
    "    print(\"\\n‚úÖ Coleta e limpeza conclu√≠das!\")\n",
    "    print(f\"üîó Total de URLs √∫nicas analisadas: {len(urls_processadas)}\")\n",
    "    print(f\"üìÑ PDFs processados com sucesso: {total_pdfs}\")\n",
    "    print(f\"üåê P√°ginas web processadas com sucesso: {total_htmls}\")\n",
    "    print(f\"üßæ Total de URLs com sucesso (conte√∫do v√°lido): {total_processadas}\")\n",
    "    print(f\"‚öôÔ∏è Documentos v√°lidos salvos: {len(documentos_finais)}\")\n",
    "\n",
    "    # Exibe porcentagens\n",
    "    if total_processadas > 0:\n",
    "        perc_pdfs = (total_pdfs / total_processadas) * 100\n",
    "        perc_htmls = (total_htmls / total_processadas) * 100\n",
    "        print(f\"üìä Distribui√ß√£o: {perc_pdfs:.1f}% PDFs | {perc_htmls:.1f}% Web\")\n",
    "    return documentos_finais\n",
    "\n",
    "print(\"Fun√ß√µes do pipeline (vers√£o corrigida e mais inteligente) definidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45149677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifique-se de que voc√™ j√° fez o upload do arquivo 'articles.csv'\n",
    "# usando o √≠cone de pasta na barra lateral esquerda.\n",
    "\n",
    "csv_file_path = 'articles.csv'\n",
    "print(f\"O script agora ir√° usar o arquivo '{csv_file_path}' que voc√™ enviou.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS: Essa parte demora em m√©dia 1793.81 segundos\n",
    "import os\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    em_colab = True\n",
    "except ImportError:\n",
    "    em_colab = False\n",
    "\n",
    "inicio_execucao = time.time()\n",
    "# Fase I: Ler lista de URLs do CSV\n",
    "if 'csv_file_path' in locals() and csv_file_path:\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_file_path,\n",
    "        source_column=\"link\",\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    docs_from_csv = loader.load()\n",
    "    print(f\"Fase I conclu√≠da: {len(docs_from_csv)} URLs carregados de '{csv_file_path}'.\")\n",
    "\n",
    "    # Fase II: Cache e limpeza inteligente\n",
    "    arquivo_cache_fase2 = 'documentos_limpos_unstructured.jsonl'\n",
    "\n",
    "    if os.path.exists(arquivo_cache_fase2):\n",
    "        documentos_carregados_e_limpos = carregar_documentos_de_jsonl(arquivo_cache_fase2)\n",
    "    else:\n",
    "        documentos_carregados_e_limpos = carregar_e_limpar_conteudo(docs_from_csv)\n",
    "        if documentos_carregados_e_limpos:\n",
    "            salvar_documentos_em_jsonl(documentos_carregados_e_limpos, arquivo_cache_fase2)\n",
    "\n",
    "    # Exemplo de documento limpo\n",
    "    if documentos_carregados_e_limpos:\n",
    "        doc_exemplo = documentos_carregados_e_limpos\n",
    "        print(\"\\n--- Exemplo de Documento Limpo (p√≥s-Unstructured) ---\")\n",
    "        print(f\"Conte√∫do (primeiros 500 caracteres): {doc_exemplo[0]}...\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "    # Disponibilizar arquivo para download se estiver no Colab\n",
    "    if em_colab and os.path.exists(arquivo_cache_fase2):\n",
    "        print(\"\\nüì• Fazendo download do arquivo gerado...\")\n",
    "        files.download(arquivo_cache_fase2)\n",
    "else:\n",
    "    print(\"Por favor, execute a C√©lula 3 para fazer o upload do arquivo CSV primeiro.\")\n",
    "\n",
    "# Marcar o fim da execu√ß√£o\n",
    "fim_execucao = time.time()\n",
    "tempo_total = fim_execucao - inicio_execucao\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Tempo total de execu√ß√£o: {tempo_total:.2f} segundos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
